{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266bd10c-d961-4156-84ad-dedb0e2bd588",
   "metadata": {},
   "source": [
    "# 3 - Neural Networks \n",
    "\n",
    "(followed from https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df961e-9b8f-4ff0-b332-772e54792616",
   "metadata": {},
   "source": [
    "Neural networks can be constructed using the $\\texttt{torch.nn}$ package. $\\texttt{nn}$ depends on autograd to define models and differentiate them. An $\\texttt{nn.Module}$ contains layers, and a method forward(input) that returns the output. It takes the input, feeds it through several layers one after the other, and then finally gives the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a653f2-ad17-4627-baad-8174026af6a0",
   "metadata": {},
   "source": [
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or weights)\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back into the network’s parameters\n",
    "- Update the weights of the network, typically using a simple update rule: weight = weight - (learning rate) * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a879e5c-7421-47af-8635-4fc4d384d8bf",
   "metadata": {},
   "source": [
    "## 3.1 - Define the Network\n",
    "Let’s define this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c31ab5e-77b3-438b-a159-21a1a4722535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)        #nn.Conv2d(input_feature, output_feature, kernel size) The input_feature receives the nº of channels of the image\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)          # nn.Linear(in_feature, out_feature)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "#Neural Network structure resume\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae854e3e-3f6d-4af3-9ce9-3fbe2f4c5589",
   "metadata": {},
   "source": [
    "We just have defined the $\\texttt{forward}$ funtion. The $\\texttt{backward}$ function (where gradients are computed) is automatically defined by you using $\\texttt{autograd}$. You can use any of the Tenso operations in the forward function.\n",
    "\n",
    "The learnable parameters of a model are returned by $\\texttt{net.parameters}$()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb55ecb-7efc-4e97-8bc2-d6332fb66ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0501e7f0-e2df-4d29-80d2-78bd2f5e4a21",
   "metadata": {},
   "source": [
    "Let’s try a random 32x32 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b47f27ed-6bde-4c47-be78-24ce2f3a0e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0215, -0.0387,  0.0161,  0.1216,  0.0799,  0.1146, -0.0740,  0.1239,\n",
      "          0.0271, -0.0503]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5242301d-1ec8-4dbf-a9fe-d463fd0da7c6",
   "metadata": {},
   "source": [
    "Zero the gradient buffers of all parameters and backprops with random gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d113bf-325c-4b86-887f-248a051acc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37725b4d-1278-412b-a9e1-d238d9124475",
   "metadata": {},
   "source": [
    "### 3.1.1 - Recap of useful definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821cf5f5-0dda-4cb1-99d4-374d1336aaf5",
   "metadata": {},
   "source": [
    "- $\\texttt{torch.Tensor}$ - A multi-dimensional array with support for autograd operations like $\\texttt{backward}$(). Also holds the gradient with respect to (w.r.t.) the tensor.\n",
    "- $\\texttt{nn.Module}$ - Neural network module. Convenient way of encapsuling parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "- $\\texttt{nn.Parameter}$ - A kinf of Tensor, that automatically registered as a parameter when assigne as an attribute to a $\\texttt{Module}$.\n",
    "- $\\texttt{autograd.Function}$ - Implements forward and backward definitions of an autograd operation. Every $\\texttt{Tensor}$ operation ceates at least a single $\\texttt{Function}$ node that connects to functions that created a $\\texttt{Tensor}$ and encodes its history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d85e39-350a-427b-aad1-57d2de61647f",
   "metadata": {},
   "source": [
    "## 3.2 - Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd63f7f-4972-4499-92f4-be2cc8b471c8",
   "metadata": {},
   "source": [
    "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target. There are several different loss functions under the nn package (see https://pytorch.org/docs/nn.html#loss-functions).A simple loss is: $\\texttt{nn.MSELoss}$ which computes the mean-squared error between the output and the target. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f70e6105-96e7-49fc-a1ba-862bcb84d0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8505, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4f5780-3439-4bd8-aab3-eecf5e8e4121",
   "metadata": {},
   "source": [
    "Now, if you follow loss in the backward direction, using its .grad_fn attribute, you will see a graph of computations that looks like this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8761ae9c-a4e8-42da-b76f-4b038b6f42af",
   "metadata": {},
   "source": [
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> flatten -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b1f06-bdb1-4822-875e-34d113628c0c",
   "metadata": {},
   "source": [
    "So, when we call $\\texttt{loss.backward}$(), the whole graph is differentiated w.r.t. the neural net parameters, and all Tensors in the graph that have $\\texttt{requires_grad=True}$ will have their .$\\texttt{rad}$ Tensor accumulated with the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51acdf23-00bd-4e00-a9c6-a8e85cb3e382",
   "metadata": {},
   "source": [
    "For illustration, lets follow a few steps backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c85e24da-7d25-46a8-b28f-be842af87779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x00000278A0F1D4F0>\n",
      "<AddmmBackward0 object at 0x00000278A0F1DCD0>\n",
      "<AccumulateGrad object at 0x00000278A0F1D4F0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f762e77-39a6-4374-8fdd-0fee57934b98",
   "metadata": {},
   "source": [
    "## 3.3 - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03966cbc-fb64-4d13-af92-988b77074ade",
   "metadata": {},
   "source": [
    "To backpropagate the error all we have to do is to $\\texttt{loss.backward}()$. But before this, you need to clear the existing gradients, else gradients will be accumulated to existing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05964caa-4167-4b62-b6f6-55edb32bbdc0",
   "metadata": {},
   "source": [
    "Now we shall call $\\texttt{loss.backward}()$, and have a look at conv1’s bias gradients before and after the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75e6ead1-4793-4cdc-8b28-bcef32b5a1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0217, -0.0114, -0.0102, -0.0043,  0.0096,  0.0071])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e95e96-e680-4d86-8d1e-94d85de86d8e",
   "metadata": {},
   "source": [
    "Now, we have seen how to use loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af176ef-ef21-496c-832b-5ab2ea2b29f9",
   "metadata": {},
   "source": [
    "$\\textbf{Useful:}$: The neural network package contains various modules and loss functions that form the building blocks of deep neural networks. A full list with documentation is here -> https://pytorch.org/docs/nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095e648-ddcd-4d1b-861e-f6515f7b013f",
   "metadata": {},
   "source": [
    "## 3.4 - Update the weights\n",
    "\n",
    "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ca16647-521a-4b3e-b154-dd6a20a68bc5",
   "metadata": {},
   "source": [
    "weight = weight - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211109a9-484a-4c27-8d3c-3d4636a460e7",
   "metadata": {},
   "source": [
    "We can implement this using simple Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4bad3d1-30c7-41b8-8f8c-a86f8c9dffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bdea93a-8bef-495d-ab2a-08517b554885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#net.parameters is:\n",
    "net.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98a7c9-6646-41a7-ba0d-9b34dcc2abec",
   "metadata": {},
   "source": [
    "However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: $\\texttt{torch.optim}$ that implements all these methods. Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8282c855-fe8b-4cd9-8567-7e91322add50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d24e3-d86d-46b9-add4-c902ef2a8cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
