{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/joankl/Solar-Neutrinos-Project/blob/main/Solar-Neutrinos-Project%20/bis_data_candidates/lifetime_calculator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWJukniMsyy8"
   },
   "source": [
    "# Notebook Dedicated to Compute the Dataset Raw Lifetime using the runID lifetime calculator\n",
    "\n",
    "The Lifetime should be calculated using the entery range of runIDs used for the analysis! No need to account for cuts, we are juts interested in the total lifetime of the available dataset. The cuts will be accountes in the selection efficiency based on MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ODvLUO82swyT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVCAfVl3tris"
   },
   "source": [
    "# Load Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load atmospheric and HS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Define the removal time in seconds ====\n",
    "hs_remove_time = 1\n",
    "atm_remove_time = 20\n",
    "\n",
    "# ====== Load Dictionary ======\n",
    "\n",
    "# Data directory pattern\n",
    "main_dir = '/home/joankl/data/solars/real_data/bisMSB/bkg_candidates/Analysis*/'\n",
    "\n",
    "# Dictionary name list to load\n",
    "dict_name_list = ['hs_dict', 'atm_dict']\n",
    "\n",
    "# Create an general empty dictionary to save the loaded dictionaries with names of dict_name_list\n",
    "general_dict = {}\n",
    "\n",
    "# Create a list with the dictionaries to be iterated\n",
    "#dict_list = [hs_dict, atm_dict, prompt_coinc_dict, delay_coinc_dict]\n",
    "\n",
    "# Iterate over each dictionary\n",
    "for dict_name_list_i in dict_name_list:\n",
    "\n",
    "    # Create list of directories for the dict_i\n",
    "    file_dir_list = glob.glob(main_dir + dict_name_list_i + '.pkl')\n",
    "\n",
    "    # Create temporal dictionary to save the data in file_dir_list. Only refresh when dict_i changes\n",
    "    temp_dir = {}\n",
    "\n",
    "    add_dict_key_ctrl = 0  # Counter to controls when keys are added to temp_dir.\n",
    "\n",
    "    # Iterare over the directory files\n",
    "    for fdir_i in file_dir_list:\n",
    "        \n",
    "        with open(fdir_i, 'rb') as f:\n",
    "            dict_i = pickle.load(f)\n",
    "\n",
    "        # Add the keys of dict_i to temp_dir. It will create the keys of the loaded dictionary one time since add_dict_key_ctrl will be > 0.\n",
    "        if add_dict_key_ctrl == 0:\n",
    "            dict_keys = list(dict_i.keys())\n",
    "            for key_i in dict_keys:\n",
    "                temp_dir[key_i] = []\n",
    "            \n",
    "            add_dict_key_ctrl += 1\n",
    "\n",
    "        # Append the values of the loaded dict_i to the tempo dict\n",
    "        for key_i in dict_keys:\n",
    "            temp_dir[key_i].append(dict_i[key_i])\n",
    "\n",
    "        # Once the file_dir_list is fully iterated, create the genara_dict keys and save the temp_dir informarion\n",
    "        general_dict[dict_name_list_i] = temp_dir\n",
    "\n",
    "# ====== Extract Data ======\n",
    "hs_dict = general_dict['hs_dict']\n",
    "atm_dict = general_dict['atm_dict']\n",
    "\n",
    "\n",
    "hs_counter = hs_dict['counter']\n",
    "atm_counter = atm_dict['counter']\n",
    "\n",
    "total_hs = int(np.sum(hs_counter))\n",
    "total_atm = int(np.sum(atm_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IL1if0C1Cnk"
   },
   "source": [
    "## Load the livetime list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lonwvxTA1KGz"
   },
   "outputs": [],
   "source": [
    "#main_dir = 'C:/Users/joanc/jupyter notebooks/solar neutrino analysis/bis_data_candidates/run_lifetime_lists/*'\n",
    "main_dir = 'run_lifetime_lists/*'\n",
    "\n",
    "flist = glob.glob(main_dir)\n",
    "\n",
    "runID_list, time1_lt, time2_lt = [], [], []\n",
    "\n",
    "for fdir in flist:\n",
    "\n",
    "  with open(fdir, \"r\", encoding=\"utf-8\") as f:\n",
    "      for linea in f:\n",
    "          if linea.strip():  # Evitar líneas vacías\n",
    "              valores = linea.split()  # Separa por espacios o tabulaciones\n",
    "              runID_list.append(int(valores[0]))\n",
    "              time1_lt.append(float(valores[1]))\n",
    "              time2_lt.append(float(valores[2]))\n",
    "\n",
    "runID_list = np.array(runID_list)\n",
    "time1_lt = np.array(time1_lt) # RAW LIVETIME (DAYS)\n",
    "time2_lt = np.array(time2_lt) # NET LIVETIME (DAYS)\n",
    "\n",
    "# RUN SELECTION:\n",
    "#runID_selection = (runID_lt >= 300700)\n",
    "\n",
    "#runID_lt = runID_lt[runID_selection]\n",
    "#time1_lt = np.array(time1_lt)[runID_selection] # RAW LIVETIME (DAYS)\n",
    "#time2_lt = np.array(time2_lt)[runID_selection] # NET LIVETIME (DAYS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRimhQA70-h9"
   },
   "source": [
    "## Load the Full Dataset runIDs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#main_dir = 'E:/Data/solars/solarnu_Realdata/bisMSB/full_runID_list/*.txt'\n",
    "main_dir = '/home/joankl/data/solars/real_data/bisMSB/all_runID_dataset_Analysis*txt'\n",
    "fdir_list = glob.glob(main_dir)\n",
    "\n",
    "runID_dataset = []\n",
    "\n",
    "for fdir in fdir_list:\n",
    "    with open(fdir) as f:\n",
    "        for line in f:\n",
    "            runID_dataset.append(int(line.strip()))\n",
    "\n",
    "runID_dataset = np.array(np.unique(runID_dataset))\n",
    "\n",
    "print(f'min. runID dataset: {np.min(runID_dataset)}')\n",
    "print(f'max. runID dataset: {np.max(runID_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min. runID dataset: 354099.0\n",
      "max. runID dataset: 371216.0\n"
     ]
    }
   ],
   "source": [
    "main_fdir = '/home/joankl/data/solars/real_data/bisMSB/first_candidates/analysis*/resume_files/'\n",
    "fdir_list = glob.glob(main_fdir)\n",
    "\n",
    "# Define the observables to grab from dataset: Save the into a dictionary\n",
    "obs_list = ['runID', 'posr_av', 'energy_corrected']\n",
    "obs_dict = {obs_i: np.array([]) for obs_i in obs_list}\n",
    "\n",
    "for fdir_i in fdir_list:\n",
    "    for var_i in obs_list:\n",
    "        obs_i = np.load(fdir_i + var_i +'.npy')\n",
    "        obs_dict[var_i] = np.append(obs_dict[var_i], obs_i)\n",
    "\n",
    "\n",
    "runID_dataset = obs_dict['runID']\n",
    "runID_dataset = np.unique(runID_dataset)\n",
    "\n",
    "print(f'min. runID dataset: {np.min(runID_dataset)}')\n",
    "print(f'max. runID dataset: {np.max(runID_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flS6DXxq3zyR"
   },
   "source": [
    "# Compute the Dataset Livetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dataset with livetime = 322.73863212103015 days\n"
     ]
    }
   ],
   "source": [
    "# Extract the net lifetime (time2_lt) where runID from data is in runID_list\n",
    "same_runID_condition = np.isin(runID_list, runID_dataset)\n",
    "\n",
    "filtered_lt = time2_lt[same_runID_condition]\n",
    "tot_dataset_lt_days = np.sum(filtered_lt)\n",
    "\n",
    "print(f'All dataset with livetime = {tot_dataset_lt_days} days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bTTseD6Zu-dJ",
    "outputId": "57e542b2-d9a0-4228-ee14-cdc37cafeb33"
   },
   "outputs": [],
   "source": [
    "# Compute the time to removed due to atm and hs (in days)\n",
    "\n",
    "atm_time = total_atm * (atm_remove_time) / (24 * 3600)\n",
    "hs_time = total_hs * (hs_remove_time) / (24 * 3600)\n",
    "\n",
    "print(f'Time removed due to atmospherics = {atm_time} days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOytZIBA758F7aHLPjMy/9l",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
